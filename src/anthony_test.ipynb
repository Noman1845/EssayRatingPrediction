{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import dask.dataframe as dd\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from features import (count_awl_words, count_complex_words, count_sentences, \n",
    "                      count_syllables, count_words, count_awl_words, \n",
    "                      tokenize_essay, lemmatize_essay, remove_stop_words,\n",
    "                      remove_punctuation, remove_special_words, count_characters,\n",
    "                      count_dale_chall_difficult_words, count_unique_lemme, get_pos_tags,\n",
    "                      count_incorrect_words, get_incorrect_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.max_rows = 60\n",
    "# pd.options.display.max_seq_items = 100\n",
    "\n",
    "# # pd.reset_option(\"display.max_rows\")\n",
    "# # pd.reset_option(\"display.max_seq_items\")\n",
    "\n",
    "# pd.options.display.max_colwidth = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv(\"../data/training_set_rel3.tsv\", sep=\"\\t\", encoding=\"ISO-8859-1\")\n",
    "# df_train = df_train.dropna(axis=1)\n",
    "# df_test = df_train#.iloc[:200]\n",
    "# df_test = df_test.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"])\n",
    "\n",
    "# # domain_score1\n",
    "# scaled_domain1_score_list = []\n",
    "# for i in np.arange(1):\n",
    "#     df_temp = df_test[df_test[\"essay_set\"]==i+1].copy() # parsing essay set\n",
    "#     domain_score = np.array(df_temp[\"domain1_score\"]).reshape(-1,1) # turning score column in array\n",
    "#     scaler = MinMaxScaler(feature_range=(-1,1)) # scaler\n",
    "#     scaler.fit(domain_score) # fitting ...\n",
    "#     df_temp[\"scaled_domain1_score\"] = scaler.transform(domain_score) # scaling the score column\n",
    "#     scaled_domain1_score_list.append(df_temp[\"scaled_domain1_score\"]) # add the scaled column to lists\n",
    "\n",
    "# df_test[\"scaled_domain1_score\"] = pd.concat(scaled_domain1_score_list) # concatenate to add all essay set columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_steps = 15\n",
    "\n",
    "# pipeline_start = time.time()\n",
    "\n",
    "# start = time.time()\n",
    "# print(f\"step 1/{total_steps}\")\n",
    "\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df_test[\"essay\"] = ddf[\"essay\"].apply(remove_special_words)\n",
    "\n",
    "# end = time.time()\n",
    "# print(\"step 1 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# print(f\"step 2/{total_steps}\")\n",
    "\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df_test[\"essay\"] = ddf[\"essay\"].apply(remove_punctuation)\n",
    "\n",
    "# end = time.time()\n",
    "# print(\"step 2 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# print(f\"step 3/{total_steps}\")\n",
    "\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df_test[\"essay_tokens\"] = ddf[\"essay\"].apply(tokenize_essay)\n",
    "\n",
    "# end = time.time()\n",
    "# print(\"step 3 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# print(f\"step 4/{total_steps}\")\n",
    "\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df_test[\"count_words\"] = ddf[\"essay_tokens\"].apply(len)\n",
    "\n",
    "# end = time.time()\n",
    "# print(\"step 4 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# print(f\"step 5/{total_steps}\")\n",
    "\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df_test[\"count_awl_words\"] = ddf[\"essay_tokens\"].apply(count_awl_words)\n",
    "\n",
    "# end = time.time()\n",
    "# print(\"step 5 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# print(f\"step 6/{total_steps}\")\n",
    "\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df_test[\"essay_tokens_wo_stopwords\"] = ddf[\"essay_tokens\"].apply(remove_stop_words)\n",
    "\n",
    "# end = time.time()\n",
    "# print(\"step 6 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# print(f\"step 7/{total_steps}\")\n",
    "\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df_test[\"lemmatized_essay\"] = ddf[\"essay\"].apply(lemmatize_essay)\n",
    "\n",
    "\n",
    "# end = time.time()\n",
    "# print(\"step 7 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# print(f\"step 8/{total_steps}\")\n",
    "\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df_test[\"count_complex_words\"] = ddf[\"lemmatized_essay\"].apply(count_complex_words)\n",
    "\n",
    "# end = time.time()\n",
    "# print(\"step 8 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# print(f\"step 9/{total_steps}\")\n",
    "\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df_test[\"count_characters\"] = ddf[\"essay\"].apply(count_characters)\n",
    "\n",
    "# end = time.time()\n",
    "# print(\"step 9 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# print(f\"step 10/{total_steps}\")\n",
    "\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df_test[\"dale_chall_words\"] = ddf[\"essay_tokens\"].apply(count_dale_chall_difficult_words)\n",
    "\n",
    "# end = time.time()\n",
    "# print(\"step 10 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# print(f\"step 11/{total_steps}\")\n",
    "\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df_test[\"count_unique_lemme\"] = ddf[\"essay_tokens\"].apply(count_unique_lemme)\n",
    "\n",
    "# end = time.time()\n",
    "# print(\"step 11 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# df = pd.DataFrame()\n",
    "# print(f\"step 12/{total_steps}\")\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df = ddf[\"essay_tokens\"].apply(get_pos_tags, meta=(pd.Series(dtype=\"object\")))\n",
    "# df = pd.json_normalize(df)\n",
    "# df_test = df_test.join(df, how=\"left\")\n",
    "# end = time.time()\n",
    "# print(\"step 12 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# print(f\"step 13/{total_steps}\")\n",
    "\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df_test[\"count_incorrect_words\"] = df_test[\"essay_tokens\"].apply(count_incorrect_words)\n",
    "# end = time.time()\n",
    "# print(\"step 13 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# print(f\"step 14/{total_steps}\")\n",
    "\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df_test[\"count_sentences\"] = df_test[\"essay\"].apply(count_sentences)\n",
    "# end = time.time()\n",
    "# print(\"step 14 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# print(f\"step 15/{total_steps}\")\n",
    "\n",
    "# ddf = dd.from_pandas(df_test, npartitions=32)\n",
    "# df_test[\"count_syllables\"] = df_test[\"essay_tokens\"].apply(count_syllables)\n",
    "# end = time.time()\n",
    "# print(\"step 15 completed in \", end - start, \" seconds\")\n",
    "\n",
    "# pipeline_end = time.time()\n",
    "# print(f\"execution of pipeline took {pipeline_end - pipeline_start} seconds\")\n",
    "\n",
    "# df_test.to_pickle(\"./processed_data.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle(\"./processed_data.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsoned = get_pos_tags(df_test[\"essay_tokens\"][0])\n",
    "\n",
    "# df = pd.json_normalize(jsoned)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_test[\"essay_tokens\"].apply(get_pos_tags)\n",
    "# df = pd.json_normalize(df)\n",
    "# df\n",
    "\n",
    "# df_test = df_test.join(df)\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test[\"count_incorrect_words\"] = df_test[\"essay_tokens\"].apply(count_incorrect_words)\n",
    "# df_test[\"get_incorrect_words\"] = df_test[\"essay_tokens\"].apply(get_incorrect_words)\n",
    "\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>scaled_domain1_score</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_awl_words</th>\n",
       "      <th>count_complex_words</th>\n",
       "      <th>count_characters</th>\n",
       "      <th>dale_chall_words</th>\n",
       "      <th>count_unique_lemme</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>...</th>\n",
       "      <th>VBN</th>\n",
       "      <th>VBP</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>WDT</th>\n",
       "      <th>WP</th>\n",
       "      <th>WP$</th>\n",
       "      <th>WRB</th>\n",
       "      <th>count_incorrect_words</th>\n",
       "      <th>count_sentences</th>\n",
       "      <th>count_syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>340</td>\n",
       "      <td>35</td>\n",
       "      <td>260</td>\n",
       "      <td>1442</td>\n",
       "      <td>73</td>\n",
       "      <td>162</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>4.000000e-01</td>\n",
       "      <td>409</td>\n",
       "      <td>47</td>\n",
       "      <td>317</td>\n",
       "      <td>1765</td>\n",
       "      <td>107</td>\n",
       "      <td>186</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>272</td>\n",
       "      <td>49</td>\n",
       "      <td>212</td>\n",
       "      <td>1185</td>\n",
       "      <td>71</td>\n",
       "      <td>142</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>484</td>\n",
       "      <td>85</td>\n",
       "      <td>393</td>\n",
       "      <td>2275</td>\n",
       "      <td>144</td>\n",
       "      <td>226</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>462</td>\n",
       "      <td>48</td>\n",
       "      <td>355</td>\n",
       "      <td>2023</td>\n",
       "      <td>88</td>\n",
       "      <td>196</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12971</th>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>792</td>\n",
       "      <td>44</td>\n",
       "      <td>594</td>\n",
       "      <td>3118</td>\n",
       "      <td>138</td>\n",
       "      <td>306</td>\n",
       "      <td>54</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12972</th>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>514</td>\n",
       "      <td>25</td>\n",
       "      <td>385</td>\n",
       "      <td>1971</td>\n",
       "      <td>93</td>\n",
       "      <td>203</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>761</td>\n",
       "      <td>66</td>\n",
       "      <td>583</td>\n",
       "      <td>3235</td>\n",
       "      <td>167</td>\n",
       "      <td>341</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>543</td>\n",
       "      <td>38</td>\n",
       "      <td>399</td>\n",
       "      <td>2181</td>\n",
       "      <td>141</td>\n",
       "      <td>234</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>460</td>\n",
       "      <td>38</td>\n",
       "      <td>331</td>\n",
       "      <td>1899</td>\n",
       "      <td>116</td>\n",
       "      <td>220</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12976 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       domain1_score  scaled_domain1_score  count_words  count_awl_words  \\\n",
       "0                  8          2.000000e-01          340               35   \n",
       "1                  9          4.000000e-01          409               47   \n",
       "2                  7          2.220446e-16          272               49   \n",
       "3                 10          6.000000e-01          484               85   \n",
       "4                  8          2.000000e-01          462               48   \n",
       "...              ...                   ...          ...              ...   \n",
       "12971             35                   NaN          792               44   \n",
       "12972             32                   NaN          514               25   \n",
       "12973             40                   NaN          761               66   \n",
       "12974             40                   NaN          543               38   \n",
       "12975             40                   NaN          460               38   \n",
       "\n",
       "       count_complex_words  count_characters  dale_chall_words  \\\n",
       "0                      260              1442                73   \n",
       "1                      317              1765               107   \n",
       "2                      212              1185                71   \n",
       "3                      393              2275               144   \n",
       "4                      355              2023                88   \n",
       "...                    ...               ...               ...   \n",
       "12971                  594              3118               138   \n",
       "12972                  385              1971                93   \n",
       "12973                  583              3235               167   \n",
       "12974                  399              2181               141   \n",
       "12975                  331              1899               116   \n",
       "\n",
       "       count_unique_lemme  CC  CD  ...  VBN  VBP  VBZ  WDT  WP  WP$  WRB  \\\n",
       "0                     162  14   0  ...    6   17   10    0   1    0    4   \n",
       "1                     186  18   5  ...    6   21    6    3   0    0    9   \n",
       "2                     142  16   2  ...    0   25    5    0   5    0    3   \n",
       "3                     226  17   0  ...    9   21   11    1   3    0    4   \n",
       "4                     196  15   5  ...    2   26   16    3   1    0    5   \n",
       "...                   ...  ..  ..  ...  ...  ...  ...  ...  ..  ...  ...   \n",
       "12971                 306  54  11  ...   11   34   19    3   4    0    8   \n",
       "12972                 203  26   4  ...    5   19    6    2   0    0    3   \n",
       "12973                 341  35   5  ...   26    9   12    9   4    0    4   \n",
       "12974                 234  22   4  ...   15    9    9    1   6    0    7   \n",
       "12975                 220  12   1  ...   10   31   17    5   2    0    5   \n",
       "\n",
       "       count_incorrect_words  count_sentences  count_syllables  \n",
       "0                         11                1              411  \n",
       "1                         13                1              534  \n",
       "2                          1                1              351  \n",
       "3                         21                1              660  \n",
       "4                         11                1              608  \n",
       "...                      ...              ...              ...  \n",
       "12971                      1                1              938  \n",
       "12972                      9                1              580  \n",
       "12973                     12                1              941  \n",
       "12974                      2                1              631  \n",
       "12975                      1                1              566  \n",
       "\n",
       "[12976 rows x 47 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test = df_test.drop(columns=[\"essay\", \"essay_id\", \"essay_set\", \"essay_tokens\", \"essay_tokens_wo_stopwords\", \"lemmatized_essay\"])\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['domain1_score', 'scaled_domain1_score', 'count_words',\n",
       "       'count_awl_words', 'count_complex_words', 'count_characters',\n",
       "       'dale_chall_words', 'count_unique_lemme', 'CC', 'CD', 'DT', 'EX', 'FW',\n",
       "       'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT',\n",
       "       'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB',\n",
       "       'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB',\n",
       "       'count_incorrect_words', 'count_sentences', 'count_syllables'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'essay_set'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_test \u001b[38;5;241m=\u001b[39m \u001b[43mdf_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43messay_set\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df_test\n",
      "File \u001b[1;32mh:\\DSTI-REPO\\EssayRatingPrediction\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:9156\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   9153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   9154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 9156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9159\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9162\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\DSTI-REPO\\EssayRatingPrediction\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[1;32mh:\\DSTI-REPO\\EssayRatingPrediction\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'essay_set'"
     ]
    }
   ],
   "source": [
    "df_test = df_test.groupby(\"essay_set\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_test,test_size = 0.2, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.loc[:, ['count_words', 'count_awl_words',\n",
    "       'count_complex_words', 'count_characters', 'dale_chall_words',\n",
    "       'count_unique_lemme', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR',\n",
    "       'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP',\n",
    "       'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG',\n",
    "       'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB',\n",
    "       'count_incorrect_words']].values\n",
    "y_train = df_train.scaled_domain1_score.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.loc[:, ['count_words', 'count_awl_words',\n",
    "       'count_complex_words', 'count_characters', 'dale_chall_words',\n",
    "       'count_unique_lemme', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR',\n",
    "       'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP',\n",
    "       'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG',\n",
    "       'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB',\n",
    "       'count_incorrect_words']].values\n",
    "y_test = df_test.scaled_domain1_score.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4123025191721.009\n"
     ]
    }
   ],
   "source": [
    "regr = LinearRegression()\n",
    "regr.fit(X=X_train, y=y_train)\n",
    "print(regr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;DecisionTreeRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.tree.DecisionTreeRegressor.html\">?<span>Documentation for DecisionTreeRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeRegressor()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "random_forrest = RandomForestRegressor()\n",
    "adaboost = AdaBoostRegressor()\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "\n",
    "random_forrest.fit(X=X_train,y=y_train)\n",
    "adaboost.fit(X=X_train,y=y_train)\n",
    "decision_tree.fit(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7753400370713625\n",
      "0.7561576782917132\n",
      "0.44719345609864214\n"
     ]
    }
   ],
   "source": [
    "print(random_forrest.score(X_test, y_test))\n",
    "print(adaboost.score(X_test, y_test))\n",
    "print(decision_tree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5h17',\n",
       " 'dickz',\n",
       " 'nuggets',\n",
       " 'm0ngoloid',\n",
       " 'assfvck3r',\n",
       " 'dog fucking',\n",
       " 'b1tchin',\n",
       " 'n1gguh',\n",
       " 'shiesterfucks',\n",
       " 'pussylickers',\n",
       " 'shittastic',\n",
       " 'fux0r',\n",
       " 'j3rk',\n",
       " 'goddamn son-of-a-bitch',\n",
       " 'wankies',\n",
       " 'fucking retard',\n",
       " 'niccer',\n",
       " 'bender',\n",
       " 'bitchass',\n",
       " 'ape shit',\n",
       " 'butt-fuckers',\n",
       " 'niggir',\n",
       " 'ass eating nob jokeys',\n",
       " 'b1tchez',\n",
       " 'dipshits',\n",
       " 'fart',\n",
       " 'ass-fucker',\n",
       " 'fukheads',\n",
       " 'cumdumpster',\n",
       " 'coons',\n",
       " 'knobjokeys',\n",
       " 'butt fucker',\n",
       " 'nigre',\n",
       " 'm0f0',\n",
       " 'prick',\n",
       " 'asses',\n",
       " 'shitterfucker',\n",
       " 'cholo',\n",
       " 'fukkuh',\n",
       " 'fellatio aficionado',\n",
       " 'darkshit',\n",
       " 'shit-ass',\n",
       " 'tongue fucking',\n",
       " 'muhfucking',\n",
       " 'm0therfvcker',\n",
       " 'ladyboys',\n",
       " 'butt-munchers',\n",
       " 'knobender',\n",
       " 'phucks',\n",
       " 'scum',\n",
       " 'shittyfuck',\n",
       " 'fcukker',\n",
       " 'ass hole',\n",
       " 'nigah',\n",
       " 'fudgepackers',\n",
       " 'fxcked',\n",
       " 'Tranny',\n",
       " 'mofcker',\n",
       " 'knobd',\n",
       " 'dumbass son-of-a-bitch',\n",
       " 'ashole',\n",
       " 'cuntelope',\n",
       " 'ja1lbait',\n",
       " 'b1tchees',\n",
       " 'dumbshit',\n",
       " 'muffdiving',\n",
       " 'nobheads',\n",
       " '4r5es',\n",
       " 'bastard',\n",
       " 'knobend',\n",
       " 'fuckass',\n",
       " 'transvestite',\n",
       " 'mothaf@cked',\n",
       " 'jiggerboo',\n",
       " 'phukked',\n",
       " 'bullshite',\n",
       " 'nigg3rs',\n",
       " 'turd',\n",
       " 'weiner',\n",
       " 'fuck',\n",
       " 'Mothafucc',\n",
       " 'nobj0key',\n",
       " 'p3nisfcukers',\n",
       " 'faggot nobjockies',\n",
       " 'wanked',\n",
       " 'sonz of bitchez',\n",
       " 'k!ke',\n",
       " 'fagging',\n",
       " 'cockskin',\n",
       " 'n0bj0cky',\n",
       " 'g@y b1tch',\n",
       " 'knob gobbler',\n",
       " 'spook',\n",
       " 'motherfvckers',\n",
       " 'blowjob',\n",
       " 'shitbrain',\n",
       " 'd!cks',\n",
       " 'packerfudgehead',\n",
       " 'mof0es',\n",
       " 'nut butter',\n",
       " 'jack off',\n",
       " 'sh3mal3',\n",
       " 'bitches',\n",
       " 'd!ldo',\n",
       " 'doosh',\n",
       " 'man chowder',\n",
       " 'sonofabitch',\n",
       " 'pigshit',\n",
       " 'phucup',\n",
       " 'arsefuck',\n",
       " '5h1t',\n",
       " 'fcktard',\n",
       " 'crazy mofos',\n",
       " 'bitcher',\n",
       " '0ral seks',\n",
       " 'shipdit',\n",
       " 'wanking',\n",
       " 'penisfucker',\n",
       " 'fuckin',\n",
       " 'dumbass mothafuckers',\n",
       " 'shitedick',\n",
       " 'crazy sob',\n",
       " 'arseshit',\n",
       " 'gay fuck',\n",
       " 'twink',\n",
       " 'fuggerz',\n",
       " 'fuckhead',\n",
       " 'dick nibbler',\n",
       " 'bi + ch',\n",
       " 'slit',\n",
       " 'spooks',\n",
       " 'shitter',\n",
       " 'butthole boy',\n",
       " 'higg@',\n",
       " 'pedo',\n",
       " 'douche',\n",
       " 'palm jockey',\n",
       " 'dicksucker',\n",
       " 'motherfxcking',\n",
       " 'c0x',\n",
       " 'punta',\n",
       " 'faggot nob jokeys',\n",
       " 'upskirts',\n",
       " 'hag',\n",
       " 'assfukka',\n",
       " 'knobgobbler',\n",
       " 'fcuks',\n",
       " 'abraham',\n",
       " 'nutsacks',\n",
       " 'wank3rs',\n",
       " 'fuck y0u',\n",
       " 'bugger off',\n",
       " 'knob-gobbler',\n",
       " 'damn',\n",
       " 'cumball',\n",
       " 'fart-fucker',\n",
       " 'fukking',\n",
       " 'mothafucking',\n",
       " 'niggas',\n",
       " 'erectoplasm',\n",
       " 'fuckheads',\n",
       " 'cuntlick',\n",
       " 'cuntness',\n",
       " 'pussy licker',\n",
       " 'cobb knobbler',\n",
       " 'phuccing',\n",
       " 'fucktards',\n",
       " 'motherfckin',\n",
       " 'knobjocky',\n",
       " 'nigg4h',\n",
       " 'dick milker',\n",
       " 'cockmunch',\n",
       " 'fuckless',\n",
       " 'dickskin',\n",
       " \"horse's asses\",\n",
       " 'shitefuck',\n",
       " '0rgasms',\n",
       " 'cum',\n",
       " 'prick-face',\n",
       " 'fuckwad',\n",
       " 'mothafuckeds',\n",
       " 'chinese-virus',\n",
       " 'motherfcking',\n",
       " 'queers',\n",
       " 'anus plug',\n",
       " 'what the fuck',\n",
       " 'penisfvckers',\n",
       " 'pedophile',\n",
       " 'mother fucker',\n",
       " 'fack',\n",
       " 'mofoes',\n",
       " 'phaggot',\n",
       " 'n1ggah',\n",
       " 'shitfacefuck',\n",
       " 'cums',\n",
       " 'fellatio',\n",
       " 'diks',\n",
       " 'jerk-0ff',\n",
       " 'kiss ass',\n",
       " 'gin jockey',\n",
       " 'gay shit',\n",
       " 'faggoting',\n",
       " 'kkk',\n",
       " 's.o.b.',\n",
       " 'wang wrangler',\n",
       " 'beaner',\n",
       " 'scumfuck',\n",
       " 'clitlickers',\n",
       " 'bang',\n",
       " 'bollockchops',\n",
       " 'jiggas',\n",
       " 'stupid hoe',\n",
       " 'penishead',\n",
       " 'dogsh1t',\n",
       " 'shitties',\n",
       " 'meat curtains',\n",
       " 'dago',\n",
       " 'cuntless',\n",
       " 'god damn',\n",
       " 'packsomefudgefucker',\n",
       " 'fagot',\n",
       " 'pussy',\n",
       " 'sum of a bitch',\n",
       " 'tickle the pickle',\n",
       " 'pussy fucker',\n",
       " 'mudderfukker',\n",
       " 'dipshit',\n",
       " 'cockhead',\n",
       " 'stfu',\n",
       " 'dumbass',\n",
       " 'buttfucker',\n",
       " 'god dammit',\n",
       " 'ass-fuckers',\n",
       " 'dumbfucks',\n",
       " 'gaytard',\n",
       " 'b1otchs',\n",
       " 'shitfck',\n",
       " 'assfucker',\n",
       " 'hustler',\n",
       " 'horseshit',\n",
       " 'fags',\n",
       " 'cocksuck',\n",
       " 'chocha',\n",
       " 'aboe',\n",
       " 'ejaculation',\n",
       " 'closet fuckhead',\n",
       " 'chink',\n",
       " 'man meat',\n",
       " 'phuq',\n",
       " 'shit-bandit',\n",
       " 'towelshithead',\n",
       " 'ja!lbait',\n",
       " 'cooterpuffing',\n",
       " 'funbags',\n",
       " 'nuckas',\n",
       " 'dumasses',\n",
       " 'fuck your',\n",
       " 'cockheads',\n",
       " 'hell',\n",
       " 'sperm',\n",
       " 'vulva',\n",
       " 'biches',\n",
       " 'packing fudgehead',\n",
       " 'fuk1ng',\n",
       " 'fker',\n",
       " 'stupid fucker',\n",
       " 'nobhead',\n",
       " 'cockface',\n",
       " 'niggass',\n",
       " 'g@ylord',\n",
       " 'n1gg@',\n",
       " 'bitchlike',\n",
       " 'slag',\n",
       " 'ass fecker',\n",
       " 'doggo style',\n",
       " 'knob-head',\n",
       " 'pillow biter',\n",
       " 'tar-baby',\n",
       " 'motherfecker',\n",
       " 'pussy licking',\n",
       " 'injun',\n",
       " 'zip in the wire',\n",
       " 'double pen',\n",
       " 'knob',\n",
       " 'motherfuckins',\n",
       " 'cunt fart',\n",
       " 'jewboy',\n",
       " 'fuckshithead',\n",
       " 'stump chewer',\n",
       " 'pissoffs',\n",
       " 'sluts',\n",
       " 'double-dong',\n",
       " 'jew',\n",
       " 'finger fuck',\n",
       " 'son-of-a-bitch',\n",
       " 'fuker',\n",
       " 'mtherfuker',\n",
       " 'cuksuker',\n",
       " 'd0ggy style',\n",
       " 'jerk off',\n",
       " 'wophead',\n",
       " 'nobbyhead',\n",
       " 'fckahz',\n",
       " 'bwc',\n",
       " 's_h_i_s',\n",
       " 'doggy_style',\n",
       " 'mongrel',\n",
       " 'niggerz',\n",
       " 'sk@nkz',\n",
       " 'puta',\n",
       " 'cock sucking boiolas',\n",
       " 'fked',\n",
       " 'knob3nd',\n",
       " 'sphincter',\n",
       " 'paki',\n",
       " 'puussy',\n",
       " 'go to hell',\n",
       " 'dumbarrassed',\n",
       " 'bumhole',\n",
       " 'anal hore',\n",
       " 'asshol3',\n",
       " 'chinavirus',\n",
       " 'tadger',\n",
       " 'phukker',\n",
       " '@ssfvcker',\n",
       " 'l@dyboy',\n",
       " 'shitty mofoes',\n",
       " 'shit licker',\n",
       " 'cock-sucker',\n",
       " 'beyoch',\n",
       " 'darktard',\n",
       " 'motherfuccas',\n",
       " '5kank',\n",
       " 'buttfuck',\n",
       " 'fvckker bunny',\n",
       " 'motherfacking',\n",
       " 'phukeds',\n",
       " 'motherfucks',\n",
       " 'c*nty',\n",
       " 'ladboyz',\n",
       " 'ass bandit',\n",
       " 'bum-bandits',\n",
       " 'higg3r',\n",
       " 'throat yogurt',\n",
       " 'bitchs',\n",
       " 'nigg@hs',\n",
       " 'sh1t',\n",
       " 'focker',\n",
       " 'm0thafucked',\n",
       " 'dyke',\n",
       " 'cocksuk',\n",
       " 'ladiboy',\n",
       " 'zipperhead',\n",
       " 'pissoff',\n",
       " 'c0ck',\n",
       " 'caca',\n",
       " 'godamnit',\n",
       " 'breast',\n",
       " 'fag bag',\n",
       " 'crotch',\n",
       " 'fingerbanged',\n",
       " 'pole licker',\n",
       " 'f@cker bunny',\n",
       " 'whor3',\n",
       " 'd1cksucker',\n",
       " 'grope',\n",
       " 'dildo',\n",
       " 'rim job',\n",
       " 'bastinado',\n",
       " 'bull shit',\n",
       " 'fuck off',\n",
       " 'wetbacks',\n",
       " 'coolie',\n",
       " 'shitface',\n",
       " 'motherfuckings',\n",
       " 'arse-shagger',\n",
       " 'shit-fucker',\n",
       " 'bollox',\n",
       " 'dickjockies',\n",
       " 'shitfucks',\n",
       " 'skanks',\n",
       " 'goddamn',\n",
       " 'man seed',\n",
       " 'slantard',\n",
       " 'twatface',\n",
       " 'fick',\n",
       " 'shitasses',\n",
       " 'phuqs',\n",
       " 'finger',\n",
       " 'oven dodger',\n",
       " 'bung hole',\n",
       " 'c00nies',\n",
       " 'shitti',\n",
       " 'nickker',\n",
       " 'fukka',\n",
       " 'knobendy',\n",
       " 'phuccer',\n",
       " 'b！tch',\n",
       " 'b1tch',\n",
       " 'coon1es',\n",
       " 'shitefulls',\n",
       " 'wang',\n",
       " 'jackass',\n",
       " 'penisfcker',\n",
       " 'sl@nteye',\n",
       " 'shitfaced',\n",
       " 'muffindiving',\n",
       " 'dicksmoker',\n",
       " 'clit',\n",
       " 'cockgobbler',\n",
       " 'fucca',\n",
       " 'mothfck',\n",
       " 'slanteye',\n",
       " 'jerk-offs',\n",
       " 'penis',\n",
       " 'ragheads',\n",
       " 'sheeeet',\n",
       " 'cyberfuccs',\n",
       " 'fuccs',\n",
       " 'motherfecka',\n",
       " 'loose',\n",
       " 'niggaz',\n",
       " 'slanteye b1tch',\n",
       " 'n1gg@hs',\n",
       " 'motherfvck',\n",
       " 'n1g3r',\n",
       " 'fag0t',\n",
       " 'jackoffs',\n",
       " 'nickas',\n",
       " 'fcuking',\n",
       " 'douche bag',\n",
       " 'shittings',\n",
       " 'muff',\n",
       " 'k1ke',\n",
       " 'batshite',\n",
       " 'fuk',\n",
       " 'anal hole',\n",
       " 'bater',\n",
       " 'nigguh',\n",
       " 'd1ckhead',\n",
       " 'ch1nk',\n",
       " 'jiggaboo',\n",
       " 'MFers',\n",
       " \"mf'ers\",\n",
       " 'jerk',\n",
       " 'shitfreak',\n",
       " 'fvck you',\n",
       " 'cock-gobbler',\n",
       " 'm0f0s',\n",
       " 'pussywhipped',\n",
       " 'bewbs',\n",
       " 'niggah',\n",
       " 'pull the pud',\n",
       " 'stupidasses',\n",
       " 'cockh3ad',\n",
       " 'muthafeckers',\n",
       " 'wop',\n",
       " 'cock droplets',\n",
       " 'm@asterbated',\n",
       " 'homo',\n",
       " 'fag asses',\n",
       " 'pig',\n",
       " 'batshit',\n",
       " 'bangbros',\n",
       " 'knobends',\n",
       " 'pedoz',\n",
       " 'cock rider',\n",
       " 'bitch',\n",
       " 'wise ass',\n",
       " 'pole sucker',\n",
       " 'mothafuck',\n",
       " 'shiesterfuckhead',\n",
       " 'dipshite',\n",
       " 'assholes',\n",
       " 'whore',\n",
       " 'klan',\n",
       " 'b1otch',\n",
       " 'butt',\n",
       " 'assfkcer',\n",
       " 'boobs',\n",
       " 'sh1t3',\n",
       " 'phucker',\n",
       " 'negro',\n",
       " 'nobjokey',\n",
       " 'f@gs',\n",
       " 'arse',\n",
       " 'fuck you',\n",
       " 'arsehole',\n",
       " 'l@dyb0i',\n",
       " 'scamfuck',\n",
       " \"f'ed\",\n",
       " 'peter',\n",
       " 'mothafuccing',\n",
       " 'coot',\n",
       " 'dog fucker',\n",
       " 'fuggings',\n",
       " 'assfuckers',\n",
       " 'bitchslap',\n",
       " 'm0thafucker',\n",
       " 'fucker',\n",
       " 'nikkas',\n",
       " 'gay',\n",
       " 'fag queen',\n",
       " 'sh1td1ck',\n",
       " 'scumfvck',\n",
       " 'jack-offs',\n",
       " 'cock sucking nob jokeys',\n",
       " 'goddamn mothafuckers',\n",
       " 'meat-sword',\n",
       " 'fuckk',\n",
       " 'cumming',\n",
       " 'cuntz',\n",
       " 'goddammit',\n",
       " 'gay fucker',\n",
       " 'fcuked',\n",
       " 'shit stain',\n",
       " 'suck',\n",
       " 'pedobear',\n",
       " 'motha fucker',\n",
       " 'trashb1tch',\n",
       " 'bondage',\n",
       " 'pricks',\n",
       " 'mothafucker',\n",
       " 'shitdicks',\n",
       " 'hooters',\n",
       " 'jackarse',\n",
       " 'bitchfuck',\n",
       " 'muddafukkas',\n",
       " 'fingerpop',\n",
       " 'sh1tdick',\n",
       " 'tit',\n",
       " 'wank',\n",
       " '@sshole',\n",
       " 'dickless',\n",
       " 'b to the inch',\n",
       " 'kidtoucher',\n",
       " 'fuckmachine',\n",
       " 'dick head',\n",
       " 'god damn it',\n",
       " 'dog fuck',\n",
       " 'd1ck',\n",
       " 'shiester',\n",
       " 'mother fucking',\n",
       " 'boll0ck',\n",
       " 'arse-bandits',\n",
       " 'anus',\n",
       " 'motherfuckkers',\n",
       " 'motherfcks',\n",
       " 'shit for brains',\n",
       " 'feg',\n",
       " 'motherfckshit',\n",
       " 'fag hag',\n",
       " 'mofos',\n",
       " 'm@derfuck',\n",
       " 'mothafuccer',\n",
       " 'cojones',\n",
       " 'piss face',\n",
       " \"bitchin'\",\n",
       " 'f@ggot',\n",
       " 'negroid',\n",
       " 'sonna bitch',\n",
       " 'get fucked',\n",
       " 'bassturd',\n",
       " 'sex',\n",
       " 'ass fuck',\n",
       " 'jerks off',\n",
       " 'bizzach',\n",
       " 'chinky',\n",
       " 'mofucker',\n",
       " 'pussy cat',\n",
       " 'fccuker',\n",
       " 'fugly',\n",
       " 'higga',\n",
       " 'wh0re',\n",
       " 'sh3male',\n",
       " 'mofuccers',\n",
       " 'pillow-biter',\n",
       " 'phucking',\n",
       " 'dolt',\n",
       " 'cuntasaurus rex',\n",
       " 'cockless',\n",
       " 'pigfucking',\n",
       " 'wet back',\n",
       " 'fuckface',\n",
       " '@ssfcker',\n",
       " 'pack my fudge',\n",
       " 'fuck up',\n",
       " 'shitblimps',\n",
       " 'motherfk',\n",
       " 'shitfuck',\n",
       " 'mothafuckas',\n",
       " 'shithead',\n",
       " 'shitbird',\n",
       " 'blacky',\n",
       " 'dumb fucker',\n",
       " 'knobeads',\n",
       " 'ass fucking nob jokeys',\n",
       " 'l@dyb0y',\n",
       " 'mothafcking',\n",
       " 'shitsful',\n",
       " 'cock-suckers',\n",
       " 'chesticles',\n",
       " 'motherfuckin',\n",
       " 'twat waffle',\n",
       " 'fatass',\n",
       " 'n1g3rz',\n",
       " 'redskin',\n",
       " 'f@gg0t',\n",
       " 'pigfuckers',\n",
       " 'shitheadfucker',\n",
       " 'fucker off',\n",
       " 'scut',\n",
       " 'phukhead',\n",
       " 'fuckem',\n",
       " 'shitsdick',\n",
       " 'jigger',\n",
       " 'dump a load',\n",
       " 'mong',\n",
       " 'cock-head',\n",
       " 'shite',\n",
       " 'wigger',\n",
       " 'bitchin',\n",
       " 'motha fucka',\n",
       " 'm0fo',\n",
       " 'knob3d',\n",
       " 'niggahs',\n",
       " 'sack',\n",
       " 'shitdick',\n",
       " 'sissy',\n",
       " 'soppy bollucks',\n",
       " 'fucs',\n",
       " 'cock munch',\n",
       " 'tits',\n",
       " 'n1gg3r',\n",
       " 'n0bhead',\n",
       " 'suck my d',\n",
       " 'ape shite',\n",
       " 'nignog',\n",
       " 'fugger',\n",
       " 'n1ggers',\n",
       " 's.o.b.s',\n",
       " 'm0therfucker',\n",
       " 'shylock',\n",
       " 'mothter fuck',\n",
       " 'shtfuk',\n",
       " 'cawk',\n",
       " 'coot coot',\n",
       " 'dothead',\n",
       " 'tossing salad',\n",
       " 'slantfreak',\n",
       " 'motherfukker',\n",
       " 'raghead',\n",
       " 'shytfeisterfuck',\n",
       " 'sand nigger',\n",
       " 'spicshit',\n",
       " 'fagshit',\n",
       " 'cooni3s',\n",
       " 'penisfvcker',\n",
       " 'k!k3',\n",
       " 'm0therfuckeds',\n",
       " 'twats',\n",
       " 'c0xux0r',\n",
       " 'foreskin',\n",
       " 'fxcking',\n",
       " 'dickvag',\n",
       " 'd1cks',\n",
       " 'cocksucking',\n",
       " 'breasticles',\n",
       " 'mfing',\n",
       " 'feck',\n",
       " 'shitfudgefucker',\n",
       " 'creampie',\n",
       " 'trashbitchez',\n",
       " 'f@k',\n",
       " 'dumb bastard',\n",
       " 'phaggots',\n",
       " 'douche canoe',\n",
       " 'schlong',\n",
       " 'flamers',\n",
       " 'fucking me',\n",
       " 'jiggaboos',\n",
       " 'ass',\n",
       " 'go fuck yourself',\n",
       " 'b1tching',\n",
       " 'carpetmuncher',\n",
       " 'assface',\n",
       " 'azzhole',\n",
       " 'dipshat',\n",
       " 'trasherbitchs',\n",
       " 'dipshitter',\n",
       " 'fcked',\n",
       " 'knobhead',\n",
       " 'phagot',\n",
       " 'sh1s',\n",
       " 'bitched',\n",
       " 'phvckings',\n",
       " 'arse gobbler',\n",
       " 'skank',\n",
       " 'retard',\n",
       " 'cocksukka',\n",
       " 'fuku',\n",
       " 'b1+ch',\n",
       " 'b1tch1ng',\n",
       " 'sheister',\n",
       " 'geebag',\n",
       " 'clusterfucking',\n",
       " 'knobjokey',\n",
       " 'mothafuckz',\n",
       " 'bollocks',\n",
       " 'mofucckers',\n",
       " 'sh1ts',\n",
       " 'pussy lick',\n",
       " 'rectum',\n",
       " 'dafuq',\n",
       " 'packingfudge',\n",
       " 'cunt ass',\n",
       " 'sons of bitches',\n",
       " 'spunk',\n",
       " 'dick-gobbler',\n",
       " 'fcuk',\n",
       " 'fuck faces',\n",
       " 'faggot mothafuckers',\n",
       " 'beat my meat',\n",
       " 'negr0',\n",
       " 'snatch licker',\n",
       " 'prick-gobbler',\n",
       " 'fckin',\n",
       " 'fuckfaces',\n",
       " 'jigga',\n",
       " 'ladyboy',\n",
       " 'wetback',\n",
       " 'god damned',\n",
       " 'azz',\n",
       " 'fukk',\n",
       " 'turdhead',\n",
       " 'fucking A',\n",
       " 'niggress',\n",
       " 'dipshidiot',\n",
       " 'c*nt',\n",
       " 'coksucka',\n",
       " 'dicks',\n",
       " 'cawk muncher',\n",
       " 'b1tchs',\n",
       " 'cocksucks',\n",
       " 'homo asses',\n",
       " 'nipple',\n",
       " 'gaylord',\n",
       " 'shat',\n",
       " 'cockmuncher',\n",
       " 'tallywacker',\n",
       " 'faggot',\n",
       " 'sons of b1tches',\n",
       " 'goddamnit',\n",
       " 'ladyb0y',\n",
       " 'higgers',\n",
       " 'biotches',\n",
       " 'jizz licker',\n",
       " 'shitehead',\n",
       " 'lady-boy',\n",
       " 'nobjokeys',\n",
       " 'nigg@',\n",
       " 'clusterfuck',\n",
       " 'n1gg3rs',\n",
       " 'phuks',\n",
       " 'jiggs',\n",
       " 'shit head',\n",
       " 'anal',\n",
       " 'nigers',\n",
       " 'nymph',\n",
       " 'beat your meat',\n",
       " 'homos',\n",
       " 'fckked',\n",
       " 'arsefucker',\n",
       " 'butt-fucker',\n",
       " 'cunt',\n",
       " 'pedophl',\n",
       " 'bastid',\n",
       " 'fuckshit',\n",
       " 'kike',\n",
       " 'shitdip',\n",
       " 'mothercker',\n",
       " 'arse-bandit',\n",
       " 'cyberfcks',\n",
       " 'nicker',\n",
       " 'mutha fucker',\n",
       " 'molester',\n",
       " 'fudgepacker',\n",
       " 'phukshit',\n",
       " 'fookuh',\n",
       " 'jack-off',\n",
       " 'phuk',\n",
       " 'fckyeah',\n",
       " 'sons-of-bitches',\n",
       " 'butt pirate',\n",
       " 'piss off fuckhead',\n",
       " 'boner',\n",
       " 'bitchhole',\n",
       " 'whoar',\n",
       " 'knob eater',\n",
       " 'cawksucker',\n",
       " 'slanty',\n",
       " 'knobes',\n",
       " 'anilingus',\n",
       " 'kneegrows',\n",
       " 'whores',\n",
       " 'motherfuckkas',\n",
       " 'carpet muncher',\n",
       " 'f@g',\n",
       " 'cockhed',\n",
       " 'jerking off',\n",
       " 'cock',\n",
       " 'niggahz',\n",
       " 'retarded',\n",
       " 'arse-hole',\n",
       " 'buttfuckers',\n",
       " 'shiesterfuck',\n",
       " 'jailbait',\n",
       " 'crows',\n",
       " 'testicles',\n",
       " 'fukkers',\n",
       " 'bollock',\n",
       " 'fuckstick',\n",
       " 'shiddick',\n",
       " 'motherfvcking',\n",
       " 'sk@nks',\n",
       " 'motherfuck',\n",
       " 'mother fuck',\n",
       " 'assfukkerz',\n",
       " 'slut hole',\n",
       " 'cock licker',\n",
       " 'pig fucker',\n",
       " 'phukup',\n",
       " 'assfuckerz',\n",
       " 'fukked',\n",
       " 'bonk',\n",
       " 'kiss my ass',\n",
       " 'arsewipe',\n",
       " 'shitfk',\n",
       " 'pole smoker',\n",
       " 'mfukk',\n",
       " 'mthrfcker',\n",
       " 'knobead',\n",
       " 'bitchy ass',\n",
       " 'fuckheaded',\n",
       " \"mf'er\",\n",
       " 'asshole fucker',\n",
       " 'kitty puncher',\n",
       " 'mother fuckers',\n",
       " 'c0cksuckers',\n",
       " 'mutherfucker',\n",
       " 'fvckwhi',\n",
       " 'niguh',\n",
       " 'mfk',\n",
       " 'ass eating boiolas',\n",
       " 'shitassfuckface',\n",
       " 'jiggerboos',\n",
       " 'sambo',\n",
       " 'vag',\n",
       " 'ladyboyz',\n",
       " 'mudderfuk',\n",
       " 'mother effer',\n",
       " 'godd@mnit',\n",
       " 'jizzfucker',\n",
       " 'shi+',\n",
       " 'f0ck',\n",
       " 'p3nisfvcker',\n",
       " '0rg@sm',\n",
       " 'dipsh1tty',\n",
       " 'godd@amn',\n",
       " 'shitblimp',\n",
       " 'shiesterfuckface',\n",
       " 'forked',\n",
       " 'fukin',\n",
       " 'higger',\n",
       " 'ass fucking boiolas',\n",
       " 'mothafvcked',\n",
       " 'pedos',\n",
       " 'm@derfuckers',\n",
       " 'pigfucker',\n",
       " '3jakulating',\n",
       " 'assmuncher',\n",
       " 'dogshits',\n",
       " 'shits',\n",
       " 'god-dam',\n",
       " 'ar5ehole',\n",
       " 'feck arse',\n",
       " 'slanteyeshit',\n",
       " 'trashbitch',\n",
       " 'gash-stabber',\n",
       " 'b0llocks',\n",
       " 'r3t@rded',\n",
       " 'trasherbitch',\n",
       " 'gook',\n",
       " 'cooter',\n",
       " 'piggyfuck',\n",
       " 'asswipe',\n",
       " 'pu55y',\n",
       " 'cunt lips',\n",
       " 'breasts',\n",
       " 'phuku',\n",
       " 'assfukah',\n",
       " '4r5ed',\n",
       " 'mothafcked',\n",
       " 'taking the piss',\n",
       " 'fudgefucker',\n",
       " 'phuck',\n",
       " 'dune coon',\n",
       " 'ballbag',\n",
       " 'BJ',\n",
       " 'phucc',\n",
       " 'butt fuck',\n",
       " 'g0ddamned',\n",
       " 'we1back',\n",
       " 'choad nectar',\n",
       " 'm0fos',\n",
       " 'double dicking',\n",
       " 'motherfucca',\n",
       " 'dick-head',\n",
       " 'molest',\n",
       " 'lesbian',\n",
       " 'fuckshitface',\n",
       " 'hebe',\n",
       " 'mothafuckins',\n",
       " 'blow a load',\n",
       " 'beeotch',\n",
       " 'shitsfuck',\n",
       " 'dlcks',\n",
       " 'bishes',\n",
       " 'nobs',\n",
       " 'sh!+',\n",
       " 'd1ckheads',\n",
       " 'cajones',\n",
       " 'porch monkey',\n",
       " 'dog shit',\n",
       " 'ape',\n",
       " 'cocksucked',\n",
       " 'Jack off',\n",
       " 'bum-driller',\n",
       " 'hoe-asses',\n",
       " 'b1tch35',\n",
       " 'shitdikk',\n",
       " 'darkass',\n",
       " 'son o bitch',\n",
       " 'ku kluxer',\n",
       " 'faggots',\n",
       " 'sh1tt',\n",
       " 'dild0',\n",
       " 'dong',\n",
       " 'cock jockey',\n",
       " 'ballsack',\n",
       " 'mothafuckasses',\n",
       " 'phukkeds',\n",
       " 'dick',\n",
       " 'tramp',\n",
       " 'gay bitch',\n",
       " 'apeshite',\n",
       " 'c0cksucked',\n",
       " 'shitfuckmotherfucker',\n",
       " 'orgasm',\n",
       " 'fuks',\n",
       " 'fackuhs',\n",
       " 'jerkoff jerking off',\n",
       " 'spanking',\n",
       " 'mofo ass',\n",
       " 'dickface',\n",
       " 'gaylords',\n",
       " 'holy fuck',\n",
       " 'cunt licker',\n",
       " 'f0cker',\n",
       " 'jizzed',\n",
       " 'useless fucker',\n",
       " 'dumb@ss',\n",
       " 'nigguhs',\n",
       " 'spic',\n",
       " 'wanks',\n",
       " 'dumbasses',\n",
       " 'l3itches',\n",
       " 'assfk',\n",
       " 'god damnit',\n",
       " 'asswhole',\n",
       " 'kiddy touch',\n",
       " 'muffindivin',\n",
       " 'phucked',\n",
       " 'tosser',\n",
       " 'fkings',\n",
       " 'cocks',\n",
       " 'cocksuka',\n",
       " 'slanteyes',\n",
       " 'cuntits',\n",
       " 'wound',\n",
       " 'salad tosser',\n",
       " 'basturd',\n",
       " 'clusterfucks',\n",
       " 'rat baztad',\n",
       " 'analplugs',\n",
       " 'cuntlickers',\n",
       " 'kikes',\n",
       " 'jizzy',\n",
       " 'coon',\n",
       " 'slant eye',\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "slur = pd.read_csv(\"./utils/profanity_en.csv\")\n",
    "slur = slur[[\"text\", \"canonical_form_1\", \"canonical_form_2\", \"canonical_form_3\"]]\n",
    "# slur = slur.replace(np.nan, '', regex=True)\n",
    "\n",
    "list_slur_wordlist = slur.values\n",
    "list_slur_wordlist = [x for sl in list_slur_wordlist for x in sl]\n",
    "set_slur_words = set(list_slur_wordlist)\n",
    "set_slur_words\n",
    "\n",
    "with op"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_erp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
